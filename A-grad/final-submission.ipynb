{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# general packages\nimport pandas as pd\nimport re, string\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# sklearn packages\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.dummy import DummyClassifier\nfrom skopt import BayesSearchCV\nfrom xgboost import XGBClassifier\n\nfrom imblearn.pipeline import Pipeline as imbPipeline\nfrom imblearn.combine import SMOTETomek\n\n#natural language packages\nfrom string import punctuation\nimport unidecode\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-13T19:29:50.584261Z","iopub.execute_input":"2021-12-13T19:29:50.584928Z","iopub.status.idle":"2021-12-13T19:29:51.840226Z","shell.execute_reply.started":"2021-12-13T19:29:50.584855Z","shell.execute_reply":"2021-12-13T19:29:51.839351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading datasets","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/declutter20v2/train_set_0520.csv\")\ndf_pred = pd.read_csv(\"../input/declutter20v2/test_set_0520.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.845049Z","iopub.execute_input":"2021-12-13T19:29:51.845273Z","iopub.status.idle":"2021-12-13T19:29:51.8705Z","shell.execute_reply.started":"2021-12-13T19:29:51.845236Z","shell.execute_reply":"2021-12-13T19:29:51.86941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have the data loaded let us take a look at it and see what columns we don't really need","metadata":{}},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.872453Z","iopub.execute_input":"2021-12-13T19:29:51.872753Z","iopub.status.idle":"2021-12-13T19:29:51.894633Z","shell.execute_reply.started":"2021-12-13T19:29:51.872719Z","shell.execute_reply":"2021-12-13T19:29:51.893521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I would guess that we don't need most of these columns so I am going to drop `ID`, `path_to_file`, and `link_to_comment`, but I will make sure to the save it to a new variable just incase.","metadata":{}},{"cell_type":"code","source":"df_train_copy = df_train.drop(['ID', 'path_to_file', 'link_to_comment'], axis = 1)\ndf_pred_copy = df_pred.drop(['ID', 'path_to_file', 'link_to_comment'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.896959Z","iopub.execute_input":"2021-12-13T19:29:51.897236Z","iopub.status.idle":"2021-12-13T19:29:51.905357Z","shell.execute_reply.started":"2021-12-13T19:29:51.897205Z","shell.execute_reply":"2021-12-13T19:29:51.90425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning & Text Pre-processing","metadata":{}},{"cell_type":"markdown","source":"### Make all text lower case","metadata":{}},{"cell_type":"code","source":"df_train_copy['type'] = df_train_copy['type'].str.lower()\ndf_train_copy['comment'] = df_train_copy['comment'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.907146Z","iopub.execute_input":"2021-12-13T19:29:51.907473Z","iopub.status.idle":"2021-12-13T19:29:51.922926Z","shell.execute_reply.started":"2021-12-13T19:29:51.907432Z","shell.execute_reply":"2021-12-13T19:29:51.921922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert `type` column to a numerical categorical","metadata":{}},{"cell_type":"code","source":"df_train_copy['type'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.924231Z","iopub.execute_input":"2021-12-13T19:29:51.925878Z","iopub.status.idle":"2021-12-13T19:29:51.936573Z","shell.execute_reply.started":"2021-12-13T19:29:51.925827Z","shell.execute_reply":"2021-12-13T19:29:51.935426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_copy['type'] = df_train_copy['type'].map({'line': 0, 'javadoc': 1, 'block':2})\ndf_train_copy['type'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.937896Z","iopub.execute_input":"2021-12-13T19:29:51.938099Z","iopub.status.idle":"2021-12-13T19:29:51.949265Z","shell.execute_reply.started":"2021-12-13T19:29:51.938076Z","shell.execute_reply":"2021-12-13T19:29:51.948607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred_copy['type'] = df_pred_copy['type'].map({'line': 0, 'javadoc': 1, 'block':2})","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.951147Z","iopub.execute_input":"2021-12-13T19:29:51.951448Z","iopub.status.idle":"2021-12-13T19:29:51.960065Z","shell.execute_reply.started":"2021-12-13T19:29:51.951409Z","shell.execute_reply":"2021-12-13T19:29:51.959178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Delete duplicates","metadata":{}},{"cell_type":"code","source":"df_train_copy.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.961621Z","iopub.execute_input":"2021-12-13T19:29:51.962458Z","iopub.status.idle":"2021-12-13T19:29:51.98301Z","shell.execute_reply.started":"2021-12-13T19:29:51.962407Z","shell.execute_reply":"2021-12-13T19:29:51.981972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_copy.drop_duplicates(subset=['comment'], keep='first', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.984379Z","iopub.execute_input":"2021-12-13T19:29:51.985004Z","iopub.status.idle":"2021-12-13T19:29:51.99629Z","shell.execute_reply.started":"2021-12-13T19:29:51.984957Z","shell.execute_reply":"2021-12-13T19:29:51.995161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_copy.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:51.997356Z","iopub.execute_input":"2021-12-13T19:29:51.997841Z","iopub.status.idle":"2021-12-13T19:29:52.00962Z","shell.execute_reply.started":"2021-12-13T19:29:51.997803Z","shell.execute_reply":"2021-12-13T19:29:52.008887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert `non-information` column of training set to numerical categorical","metadata":{}},{"cell_type":"code","source":"df_train_copy['non-information'] = df_train_copy['non-information'].map({'no': 0, 'yes': 1})","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.010627Z","iopub.execute_input":"2021-12-13T19:29:52.011408Z","iopub.status.idle":"2021-12-13T19:29:52.024112Z","shell.execute_reply.started":"2021-12-13T19:29:52.011368Z","shell.execute_reply":"2021-12-13T19:29:52.023233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_copy['non-information'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.026921Z","iopub.execute_input":"2021-12-13T19:29:52.027573Z","iopub.status.idle":"2021-12-13T19:29:52.040246Z","shell.execute_reply.started":"2021-12-13T19:29:52.027517Z","shell.execute_reply":"2021-12-13T19:29:52.039214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So it appears that the training set is a little imbalanced towards non-informative comments, this will be dealt with by using SMOTE sampling. ","metadata":{}},{"cell_type":"markdown","source":"### Drop any rows with null values","metadata":{}},{"cell_type":"code","source":"df_train_copy.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.042004Z","iopub.execute_input":"2021-12-13T19:29:52.042319Z","iopub.status.idle":"2021-12-13T19:29:52.057796Z","shell.execute_reply.started":"2021-12-13T19:29:52.042279Z","shell.execute_reply":"2021-12-13T19:29:52.056767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred_copy.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.059546Z","iopub.execute_input":"2021-12-13T19:29:52.060034Z","iopub.status.idle":"2021-12-13T19:29:52.074564Z","shell.execute_reply.started":"2021-12-13T19:29:52.059989Z","shell.execute_reply":"2021-12-13T19:29:52.07345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like `df_train_copy` has some null values, let's drop those.\\\nThe null values in `df_pred_copy` come from the `expected` column which will later be replaced by our predictions so that isn't an issue.","metadata":{}},{"cell_type":"code","source":"df_train_copy.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.075773Z","iopub.execute_input":"2021-12-13T19:29:52.076289Z","iopub.status.idle":"2021-12-13T19:29:52.085589Z","shell.execute_reply.started":"2021-12-13T19:29:52.076222Z","shell.execute_reply":"2021-12-13T19:29:52.084516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_copy.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.087171Z","iopub.execute_input":"2021-12-13T19:29:52.087784Z","iopub.status.idle":"2021-12-13T19:29:52.102819Z","shell.execute_reply.started":"2021-12-13T19:29:52.087738Z","shell.execute_reply":"2021-12-13T19:29:52.101979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No more null values in `df_train_copy`.\\\nLet's see what our dataframes look like now.","metadata":{}},{"cell_type":"code","source":"df_train_copy","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.104011Z","iopub.execute_input":"2021-12-13T19:29:52.104873Z","iopub.status.idle":"2021-12-13T19:29:52.125502Z","shell.execute_reply.started":"2021-12-13T19:29:52.104835Z","shell.execute_reply":"2021-12-13T19:29:52.124517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred_copy","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.12717Z","iopub.execute_input":"2021-12-13T19:29:52.127542Z","iopub.status.idle":"2021-12-13T19:29:52.142362Z","shell.execute_reply.started":"2021-12-13T19:29:52.127488Z","shell.execute_reply":"2021-12-13T19:29:52.141569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking good so far, seems that the only remaining thing left to do is extract some simple features out the the `comment` column like word count, character count, etc.\\\nThen we need to processing the `comment` column into word vectors.","metadata":{}},{"cell_type":"code","source":"# count number of characters \ndef count_chars(text):\n    return len(text)\n\n# count number of words \ndef count_words(text):\n    return len(text.split())\n\n# count of stopwords\ndef count_stopwords(text):\n    stop_words = set(stopwords.words('english'))  \n    word_tokens = word_tokenize(text)\n    stopwords_x = [w for w in word_tokens if w in stop_words]\n    return len(stopwords_x)\n\ndef contains_annotation(text):\n    if '@' in text:\n        return 1\n    else:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.143367Z","iopub.execute_input":"2021-12-13T19:29:52.143674Z","iopub.status.idle":"2021-12-13T19:29:52.1586Z","shell.execute_reply.started":"2021-12-13T19:29:52.143641Z","shell.execute_reply":"2021-12-13T19:29:52.157521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_copy['char_count'] = df_train_copy['comment'].apply(lambda x:count_chars(x))\ndf_train_copy['word_count'] = df_train_copy['comment'].apply(lambda x:count_words(x))\ndf_train_copy['stop_word_count'] = df_train_copy['comment'].apply(lambda x:count_stopwords(x))\ndf_train_copy['contains_annotation'] = df_train_copy['comment'].apply(lambda x:contains_annotation(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.160323Z","iopub.execute_input":"2021-12-13T19:29:52.161189Z","iopub.status.idle":"2021-12-13T19:29:52.6035Z","shell.execute_reply.started":"2021-12-13T19:29:52.161151Z","shell.execute_reply":"2021-12-13T19:29:52.602757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred_copy['char_count'] = df_pred_copy['comment'].apply(lambda x:count_chars(x))\ndf_pred_copy['word_count'] = df_pred_copy['comment'].apply(lambda x:count_words(x))\ndf_pred_copy['stop_word_count'] = df_pred_copy['comment'].apply(lambda x:count_stopwords(x))\ndf_pred_copy['contains_annotation'] = df_pred_copy['comment'].apply(lambda x:contains_annotation(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.604471Z","iopub.execute_input":"2021-12-13T19:29:52.605096Z","iopub.status.idle":"2021-12-13T19:29:52.709485Z","shell.execute_reply.started":"2021-12-13T19:29:52.605061Z","shell.execute_reply":"2021-12-13T19:29:52.708612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_train_copy.head(), df_pred_copy.head())","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.71071Z","iopub.execute_input":"2021-12-13T19:29:52.710933Z","iopub.status.idle":"2021-12-13T19:29:52.731931Z","shell.execute_reply.started":"2021-12-13T19:29:52.710907Z","shell.execute_reply":"2021-12-13T19:29:52.731022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have generated some more features from the `comment` column it's time to get our pre-processing pipeline set up.\\\n# Set up helper functions and pipeline","metadata":{}},{"cell_type":"code","source":"#define our stop words\nstop_words_nltk = stopwords.words('english')\n\n# list of word types (nouns and adjectives) to leave in the text\ndefTags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJS', 'JJR']#, 'RB', 'RBS', 'RBR', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n\n# functions to determine the type of a word\ndef is_noun(tag):\n    return tag in ['NN', 'NNS', 'NNP', 'NNPS']\n\ndef is_verb(tag):\n    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n\ndef is_adverb(tag):\n    return tag in ['RB', 'RBR', 'RBS']\n\ndef is_adjective(tag):\n    return tag in ['JJ', 'JJR', 'JJS']\n\n# transform tag forms\ndef penn_to_wn(tag):\n    if is_adjective(tag):\n        return nltk.stem.wordnet.wordnet.ADJ\n    elif is_noun(tag):\n        return nltk.stem.wordnet.wordnet.NOUN\n    elif is_adverb(tag):\n        return nltk.stem.wordnet.wordnet.ADV\n    elif is_verb(tag):\n        return nltk.stem.wordnet.wordnet.VERB\n    return nltk.stem.wordnet.wordnet.NOUN\n\n# lemmatizer + tokenizer (+ stemming) class\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n        # we define (but not use) a stemming method, uncomment the last line in __call__ to get stemming tooo\n        self.stemmer = nltk.stem.SnowballStemmer('english') \n    def __call__(self, doc):\n        # pattern for numbers | words of length=2 | punctuations | words of length=1\n        pattern = re.compile(r'[0-9]+|\\b[\\w]{2,2}\\b|[%.,_`!\"&?\\')({~@;:#}+-]+|\\b[\\w]{1,1}\\b')\n        # tokenize document\n        doc_tok = word_tokenize(doc)\n        #filter out patterns from words\n        doc_tok = [x for x in doc_tok if x not in stop_words_nltk]\n        doc_tok = [pattern.sub('', x) for x in doc_tok]\n        # get rid of anything with length=1\n        doc_tok = [x for x in doc_tok if len(x) > 1]\n        # position tagging\n        doc_tagged = nltk.pos_tag(doc_tok)\n        # selecting nouns and adjectives\n        doc_tagged = [(t[0], t[1]) for t in doc_tagged if t[1] in defTags]\n        # preparing lemmatization\n        doc = [(t[0], penn_to_wn(t[1])) for t in doc_tagged]\n        # lemmatization\n        doc = [self.wnl.lemmatize(t[0], t[1]) for t in doc]\n        # uncomment if you want stemming as well\n        #doc = [self.stemmer.stem(x) for x in doc]\n        return doc","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.733548Z","iopub.execute_input":"2021-12-13T19:29:52.734179Z","iopub.status.idle":"2021-12-13T19:29:52.747237Z","shell.execute_reply.started":"2021-12-13T19:29:52.734135Z","shell.execute_reply":"2021-12-13T19:29:52.74652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec_tdidf = TfidfVectorizer(ngram_range=(1,1), analyzer='word', #stop_words=stop_words1, \n                                               norm='l2', tokenizer=LemmaTokenizer())","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.748252Z","iopub.execute_input":"2021-12-13T19:29:52.748733Z","iopub.status.idle":"2021-12-13T19:29:52.763116Z","shell.execute_reply.started":"2021-12-13T19:29:52.748698Z","shell.execute_reply":"2021-12-13T19:29:52.762004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = XGBClassifier(random_state=42, seed=2, n_estimators=300, use_label_encoder=False, eval_metric='logloss')","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.765493Z","iopub.execute_input":"2021-12-13T19:29:52.766293Z","iopub.status.idle":"2021-12-13T19:29:52.776103Z","shell.execute_reply.started":"2021-12-13T19:29:52.76624Z","shell.execute_reply":"2021-12-13T19:29:52.775051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sm = SMOTETomek(random_state=42, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.778101Z","iopub.execute_input":"2021-12-13T19:29:52.77845Z","iopub.status.idle":"2021-12-13T19:29:52.790517Z","shell.execute_reply.started":"2021-12-13T19:29:52.778406Z","shell.execute_reply":"2021-12-13T19:29:52.78949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations on\n    Use on text columns in the data\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None, *parg, **kwarg):\n        return self\n\n    def transform(self, X):\n        # returns the input as a string\n        return X[self.key]\n    \nclass NumberSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations on\n    Use on numeric columns in the data\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # returns the input as a dataframe\n        return X[[self.key]]","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.791996Z","iopub.execute_input":"2021-12-13T19:29:52.792244Z","iopub.status.idle":"2021-12-13T19:29:52.803975Z","shell.execute_reply.started":"2021-12-13T19:29:52.792206Z","shell.execute_reply":"2021-12-13T19:29:52.802954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_stats(preds, target, labels, sep='-', sep_len=40, fig_size=(10,8)):\n    print('Accuracy = %.3f' % metrics.accuracy_score(target, preds))\n    print(sep*sep_len)\n    print('Classification report:')\n    print(metrics.classification_report(target, preds))\n    print(sep*sep_len)\n    print('Confusion matrix')\n    cm=metrics.confusion_matrix(target, preds)\n    cm = cm / np.sum(cm, axis=1)[:,None]\n    sns.set(rc={'figure.figsize':fig_size})\n    sns.heatmap(cm, \n        xticklabels=labels,\n        yticklabels=labels,\n           annot=True, cmap = 'YlGnBu')\n    plt.pause(0.05)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.807446Z","iopub.execute_input":"2021-12-13T19:29:52.80802Z","iopub.status.idle":"2021-12-13T19:29:52.817266Z","shell.execute_reply.started":"2021-12-13T19:29:52.807976Z","shell.execute_reply":"2021-12-13T19:29:52.816211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have all of our helpers set up it's time to get the pipeline set up.\\\nFirst we will start with a our feature selector pipelines.\\\n`text` is a pipeline to select textual features. In our case that is only comment column.\\","metadata":{}},{"cell_type":"code","source":"text = Pipeline([('selector', TextSelector(key='comment')), ('vectorizer', vec_tdidf)])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.81879Z","iopub.execute_input":"2021-12-13T19:29:52.819295Z","iopub.status.idle":"2021-12-13T19:29:52.829869Z","shell.execute_reply.started":"2021-12-13T19:29:52.819251Z","shell.execute_reply":"2021-12-13T19:29:52.828922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we will create one pipeline for each of our numerical features:\n* `type`\n* `begin_line`\n* `char_count`\n* `word_count`\n* `stop_word_count`","metadata":{}},{"cell_type":"code","source":"types =  Pipeline([('selector', NumberSelector(key='stop_word_count')),])\n\nline_num = Pipeline([('selector', NumberSelector(key='begin_line')),])\n\nchars = Pipeline([('selector', NumberSelector(key='char_count')),])\n\nwords = Pipeline([('selector', NumberSelector(key='word_count')),])\n\nstop_words =  Pipeline([('selector', NumberSelector(key='stop_word_count')),])\n\nannotation = Pipeline([('selector', NumberSelector(key='contains_annotation')),])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.831284Z","iopub.execute_input":"2021-12-13T19:29:52.831998Z","iopub.status.idle":"2021-12-13T19:29:52.84819Z","shell.execute_reply.started":"2021-12-13T19:29:52.831948Z","shell.execute_reply":"2021-12-13T19:29:52.847033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here comes the tricky part. To combine all feature, we use the FeatureUnion object. That makes sure there will not be any errors from combining text and number based inputs.","metadata":{}},{"cell_type":"code","source":"feats = FeatureUnion([('comment', text),\n                      ('type', types),\n                      ('begin_line', line_num),\n                      ('char_count', chars),\n                      ('word_count', words),\n                      ('stop_word_count', stop_words),\n                      ('contains_annotation', annotation)\n                      ])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.849731Z","iopub.execute_input":"2021-12-13T19:29:52.850228Z","iopub.status.idle":"2021-12-13T19:29:52.861504Z","shell.execute_reply.started":"2021-12-13T19:29:52.850175Z","shell.execute_reply":"2021-12-13T19:29:52.860804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can combine the input features and the classifier into a single pipeline.","metadata":{}},{"cell_type":"code","source":"#pipe = imbPipeline([('feats', feats),('smote', sm),('clf',clf)])\npipe = Pipeline([('feats', feats),('clf',clf)])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.862877Z","iopub.execute_input":"2021-12-13T19:29:52.863371Z","iopub.status.idle":"2021-12-13T19:29:52.87404Z","shell.execute_reply.started":"2021-12-13T19:29:52.863327Z","shell.execute_reply":"2021-12-13T19:29:52.873099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that our pipeline is build we can start hyperparameter tuning.\n# Hyper parameter tuning\nFirst we need to split the training data into a training set and a test set.\\\nThis is to determine if our hyper parameter tuning is actually improving our performance.","metadata":{}},{"cell_type":"code","source":"# split the data into train and test\ncombined_features = ['comment', 'type', 'begin_line', 'char_count', 'word_count', 'stop_word_count', 'contains_annotation']\ntarget = 'non-information'\n\nX_train, X_test, y_train, y_test = train_test_split(df_train_copy[combined_features], df_train_copy[target], test_size=0.33, random_state=42, stratify=df_train_copy[target])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.87532Z","iopub.execute_input":"2021-12-13T19:29:52.87561Z","iopub.status.idle":"2021-12-13T19:29:52.895133Z","shell.execute_reply.started":"2021-12-13T19:29:52.875577Z","shell.execute_reply":"2021-12-13T19:29:52.894095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we have to define the parameters we are going to tune. This is down below.","metadata":{}},{"cell_type":"code","source":"# definition of parameter grid to scan through\nparam_space = {'clf__colsample_bytree': [1], \n               'clf__n_estimators': [50], \n               'clf__subsample': [0.6],\n               'clf__eta': [0.01, 0.15, 0.2, 0.25, 0.3, 0.35]}\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.896457Z","iopub.execute_input":"2021-12-13T19:29:52.896992Z","iopub.status.idle":"2021-12-13T19:29:52.902369Z","shell.execute_reply.started":"2021-12-13T19:29:52.896951Z","shell.execute_reply":"2021-12-13T19:29:52.901436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to do the grid search through our parameters.","metadata":{}},{"cell_type":"code","source":"# grid search cross validation instantiation\ngrid_search = GridSearchCV(estimator = pipe, param_grid = param_space, cv = 5, scoring = 'accuracy', n_jobs = -1, verbose = 0, return_train_score=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.903412Z","iopub.execute_input":"2021-12-13T19:29:52.903658Z","iopub.status.idle":"2021-12-13T19:29:52.914717Z","shell.execute_reply.started":"2021-12-13T19:29:52.903628Z","shell.execute_reply":"2021-12-13T19:29:52.91381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:29:52.917294Z","iopub.execute_input":"2021-12-13T19:29:52.917837Z","iopub.status.idle":"2021-12-13T19:35:00.419999Z","shell.execute_reply.started":"2021-12-13T19:29:52.917797Z","shell.execute_reply":"2021-12-13T19:35:00.418902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.cv_results_['mean_train_score']","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:00.421489Z","iopub.execute_input":"2021-12-13T19:35:00.421924Z","iopub.status.idle":"2021-12-13T19:35:00.428859Z","shell.execute_reply.started":"2021-12-13T19:35:00.421893Z","shell.execute_reply":"2021-12-13T19:35:00.42777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.cv_results_['mean_test_score']","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:00.430188Z","iopub.execute_input":"2021-12-13T19:35:00.430501Z","iopub.status.idle":"2021-12-13T19:35:00.444334Z","shell.execute_reply.started":"2021-12-13T19:35:00.430461Z","shell.execute_reply":"2021-12-13T19:35:00.443589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:00.449034Z","iopub.execute_input":"2021-12-13T19:35:00.449619Z","iopub.status.idle":"2021-12-13T19:35:00.458166Z","shell.execute_reply.started":"2021-12-13T19:35:00.449574Z","shell.execute_reply":"2021-12-13T19:35:00.457193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_test = grid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:00.4592Z","iopub.execute_input":"2021-12-13T19:35:00.459785Z","iopub.status.idle":"2021-12-13T19:35:00.472192Z","shell.execute_reply.started":"2021-12-13T19:35:00.459729Z","shell.execute_reply":"2021-12-13T19:35:00.471285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test stats\npreds = clf_test.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:00.473457Z","iopub.execute_input":"2021-12-13T19:35:00.473981Z","iopub.status.idle":"2021-12-13T19:35:00.925229Z","shell.execute_reply.started":"2021-12-13T19:35:00.473934Z","shell.execute_reply":"2021-12-13T19:35:00.924545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_stats(y_test, preds, clf_test.classes_)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:00.92624Z","iopub.execute_input":"2021-12-13T19:35:00.92851Z","iopub.status.idle":"2021-12-13T19:35:01.288275Z","shell.execute_reply.started":"2021-12-13T19:35:00.928459Z","shell.execute_reply":"2021-12-13T19:35:01.287464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting if comments are informative or not on new data (`df_pred_copy`)","metadata":{}},{"cell_type":"code","source":"combined_features = ['comment', 'type', 'begin_line', 'char_count', 'word_count', 'stop_word_count', 'contains_annotation']\nX_pred = df_pred_copy[combined_features]\nX_pred","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:01.289613Z","iopub.execute_input":"2021-12-13T19:35:01.290085Z","iopub.status.idle":"2021-12-13T19:35:01.307356Z","shell.execute_reply.started":"2021-12-13T19:35:01.290051Z","shell.execute_reply":"2021-12-13T19:35:01.306424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_preds = clf_test.predict(X_pred)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:01.308739Z","iopub.execute_input":"2021-12-13T19:35:01.309181Z","iopub.status.idle":"2021-12-13T19:35:01.596804Z","shell.execute_reply.started":"2021-12-13T19:35:01.309144Z","shell.execute_reply":"2021-12-13T19:35:01.596098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_series = pd.Series(new_preds).map({0: 'no', 1: 'yes'})","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:01.597818Z","iopub.execute_input":"2021-12-13T19:35:01.60005Z","iopub.status.idle":"2021-12-13T19:35:01.60601Z","shell.execute_reply.started":"2021-12-13T19:35:01.599995Z","shell.execute_reply":"2021-12-13T19:35:01.605432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = {'ID': df_pred['ID'], 'Predicted': pred_series}\nsubmission = pd.DataFrame(data=d)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T19:35:01.606871Z","iopub.execute_input":"2021-12-13T19:35:01.607382Z","iopub.status.idle":"2021-12-13T19:35:01.623833Z","shell.execute_reply.started":"2021-12-13T19:35:01.607342Z","shell.execute_reply":"2021-12-13T19:35:01.622775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\n* https://www.kaggle.com/diveki/classification-with-nlp-xgboost-and-pipelines\n* https://www.analyticsvidhya.com/blog/2021/04/a-guide-to-feature-engineering-in-nlp/\n    * https://github.com/mohdahmad242/Feature-Engineering-in-NLP/blob/main/Feature_engineering_NLP.ipynb\n* https://github.com/scikit-learn-contrib/imbalanced-learn","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}